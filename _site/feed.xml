<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-07-22T13:22:35+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>GRaM Workshop
</subtitle><entry><title type="html">Grasping Graphormer : Assessing Transformer Performance for Graph Representation</title><link href="http://localhost:4000/blog/2024/graphormer/" rel="alternate" type="text/html" title="Grasping Graphormer : Assessing Transformer Performance for Graph Representation" /><published>2024-06-30T00:00:00+02:00</published><updated>2024-06-30T00:00:00+02:00</updated><id>http://localhost:4000/blog/2024/graphormer</id><content type="html" xml:base="http://localhost:4000/blog/2024/graphormer/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The Transformer architecture has revolutionized sequence modelling. 
Its versatility is demonstrated by its application in various domains, from natural language processing to computer vision to even reinforcement learning. 
With its strong ability to learn rich representations across domains, it seems natural that the power of the transformer can be adapted to graphs.</p>

<p>The main challenge with applying a transformer to graph data is that there is no obvious sequence-based representation of graphs. 
Graphs are commonly represented by adjacency matrices or lists, which lack inherent order and are thus unsuitable for transformers.</p>

<p>The primary reason for finding a sequence-based representation of a graph is to combine the advantages of a transformer (such as its high scalability) with the ability of graphs to capture non-sequential and multidimensional relationships. 
Graph Neural Networks (GNNs) employ various constraints during training, such as enforcing valency limits when generating molecules. 
However, choosing such constraints may not be as straightforward for other problems. 
With Graphormer, we can apply these very constraints in a simpler manner, analogous to applying a causal mask in a transformer. 
This can also aid in discovering newer ways to apply constraints in GNNs by presenting existing concepts in an intuitive manner.</p>

<p>Graphormer introduces Centrality Encoding to capture the node importance, Spatial Encoding to capture the structural relations, and Edge Encoding to capture the edge features. In addition to this, Graphormer makes other architectures easier to implement by making various existing architecture special cases of Graphormer, with the performance to boot.</p>

<hr />

<h2 id="preliminaries">Preliminaries</h2>

<ul>
  <li>
    <p><strong>Graph Neural Networks (GNNs)</strong>: Consider a graph \(G = \{V, E\}\) where \(V = \{v_1, v_2, \cdots, v_n\}\) and \(n = |V|\) is the number of nodes. 
Each node \(v_i\) has a feature vector \(x_i\). 
Modern GNNs update node representations iteratively by aggregating information from neighbours. 
The representation of node \(v_i\) at layer \(l\) is \(h^{(l)}_i\), with \(h_i^{(0)} = x_i\). 
The aggregation and combination at layer \(l\) are defined as: 
\(a_{i}^{(l)}=\text{AGGREGATE}^{(l)}\left(\left\{h_{j}^{(l-1)}: j \in \mathcal{N}(v_i)\right\}\right)\) 
\(h_{i}^{(l)}=\text{COMBINE}^{(l)}\left(h_{i}^{(l-1)}, a_{i}^{(l)}\right)\) 
where \(\mathcal{N}(v_i)\) is the set of first or higher-order neighbours of \(v_i\). 
Common aggregation functions include MEAN, MAX, and SUM. 
The COMBINE function fuses neighbor information into the node representation.</p>
  </li>
  <li>
    <p><strong>Graph Level Representation (READOUT)</strong>: In general, a READOUT function is any function of the final-layer node representations from a GNN that we can use as a graph-level representation. So \(h_G = \text{READOUT}(\{h_i^{(L)}\})\) is a graph-level representation. 
A MEAN-READOUT is a simple example of a READOUT function, where the graph-level representation is the mean of the node representations.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/gnn-480.webp 480w,/assets/img/2024-06-30-graphormer/gnn-800.webp 800w,/assets/img/2024-06-30-graphormer/gnn-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/gnn.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A message-passing neural network. Note how the node states flow from outer to inner layers, with pooling at each step to update states.<d-cite key="GoogleResearch"></d-cite>
</div>

<ul>
  <li><strong>Transformer</strong>: The Transformer architecture comprises layers with two main components: a self-attention module and a position-wise feed-forward network (FFN). 
Let \(H = [h_1^\top, \cdots, h_n^\top]^\top\in ℝ^{n\times d}\) be the input to the self-attention module, where \(d\) is the hidden dimension and \(h_i\in ℝ^{1\times d}\) is the hidden representation at position \(i\). 
The input \(H\) is projected using matrices \(W_Q\inℝ^{d\times d_K}, W_K\inℝ^{d\times d_K}\), and \(W_V\inℝ^{d\times d_V}\) to obtain representations \(Q, K, V\). Self-attention is computed as:
\(Q = HW_Q,\ K = HW_K,\ V = HW_V,\ A = \frac{QK^\top}{\sqrt{d_K}},\ Attn(H) = \text{softmax}(A)V\)
where \(A\) captures the similarity between queries and keys. 
This self-attention mechanism allows the model to understand relevant information in the sequence comprehensively.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/head-view-480.webp 480w,/assets/img/2024-06-30-graphormer/head-view-800.webp 800w,/assets/img/2024-06-30-graphormer/head-view-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/head-view.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    An illustration of the attention mechanism. Notice how each word(or token) can attend to different parts of the sequence, forward or backward.<d-cite key="Vig2024"></d-cite>.
</div>

<hr />

<h2 id="graphormer">Graphormer</h2>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm-9 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/arch-480.webp 480w,/assets/img/2024-06-30-graphormer/arch-800.webp 800w,/assets/img/2024-06-30-graphormer/arch-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="-480.webp 480w,-800.webp 800w,-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Diagram of the Graphormer Architechture.<d-cite key="ying2021transformersreallyperformbad"></d-cite>
</div>

<h3 id="centrality-encoding">Centrality Encoding</h3>

<p>In a sequence modelling task, Attention captures the semantic correlations between the nodes (tokens).
The goal of this encoding is to capture the most important nodes in the graph.
This is important information as important (heavily connected) nodes affect 
Let’s take an example.
Say we want to compare airports and find which one is the largest.
We need a common metric to compare them, so we take the sum of the total daily incoming and outgoing flights, giving us the busiest airports. 
This is what the algorithm is doing logically to identify the ‘busiest’ nodes.
Similarly, these learnable vectors help the model to flag these important nodes in the graph.
All this culminates in better performance for graph-based tasks such as molecule generation.</p>

<p>This is the Centrality Encoding equation, given as:</p>

\[h_{i}^{(0)} = x_{i} + z^{-}_{deg^{-}(v_{i})} + z^{+}_{deg^{+}(v_{i})}\]

<p>Let’s analyse this term by term:</p>

<ul>
  <li>\(h_{i}^{(0)}\) - Representation (\(h\)) of vertice i (\(v_{i}\)) at the 0th layer (first input)</li>
  <li>\(x_{i}\) - Feature vector of vertice i (\(v_{i}\))</li>
  <li>\(z^{-}_{deg^{-}(v_{i})}\) - Learnable embedding vector (\(z\)) of the indegree (\(deg^{-}\)) of vertice i (\(v_{i}\))</li>
  <li>\(z^{+}_{deg^{+}(v_{i})}\) - Learnable embedding vector (\(z\)) of the outdegree (\(deg^{+}\)) of vertice i (\(v_{i}\))</li>
</ul>

<p>This is an excerpt of the code used to compute the Centrality Encoding</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">in_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_in_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
<span class="bp">self</span><span class="p">.</span><span class="n">out_degree_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_out_degree</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Intial node feature computation.
</span><span class="n">node_feature</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_feature</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">in_degree_encoder</span><span class="p">(</span><span class="n">in_degree</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_degree_encoder</span><span class="p">(</span><span class="n">out_degree</span><span class="p">))</span>
</code></pre></div></div>

<hr />

<h3 id="spatial-encoding">Spatial Encoding</h3>

<p>There are several methods for encoding the position information of the tokens in a sequence.
In a graph, however, there is a problem. 
Graphs consist of nodes (analogous to tokens) connected with edges in a non-linear, multi-dimensional space. 
There’s no inherent notion of an “ordering” or a “sequence” in its structure, but as with positional information, it’ll be helpful if we inject some sort of structural information when we process the graph.</p>

<p>The authors propose a novel encoding called <em>Spatial Encoding</em>. 
Take a pair of nodes (analogous to tokens) as input and output a scalar value as a function, \(\phi(v_i, v_j)\). The authors choose \(\phi(v_i, v_j)\) to be shortest path distance (SPD) between the nodes. 
This scalar value is then added to the element corresponding to the operation between the two nodes in the Query-Key product matrix.</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i, v_j)}\]

<p>The above equation shows the modified computation of the Query-Key Product matrix. 
Notice that the additional term \(b_{\phi(v_i, v_j)}\) a learnable scalar value, is just an embedding look-up, and acts like a bias term.
Since this structural information is independent of which layer of our model is using it, we share this value across all layers.</p>

<p>The benefits of using such an encoding are:</p>
<ol>
  <li>Our receptive field has effectively increased, as we are no longer limited to the information from our neighbours, as is what happens in conventional message-passing networks.</li>
  <li>The model determines the best way to adaptively attend to the structural information. 
For example, if the scalar valued function is a decreasing function for a given node, we know that the nodes closer to our node are more important than the ones farther away.</li>
</ol>

<hr />

<h3 id="edge-encoding">Edge Encoding</h3>

<p>Graphormer’s edge encoding method significantly enhances the way the model incorporates structural features from graph edges into its attention mechanism. 
The prior approaches either add edge features to node features or use them during aggregation, propagating the edge information only to associated nodes. 
Graphormer’s approach ensures that edges play a vital role in the overall node correlation.</p>

<p>Initially, node features \((h_i, h_j)\) and edge features \((x_{e_n})\) from the shortest path between nodes are processed. 
For each pair of nodes \((v_i, v_j)\), the edge features on the shortest path \(SP_{ij}\) are averaged after being weighted by learnable embeddings \((w^E_n)\), this results in the edge encoding \(c_{ij}\):</p>

\[c_{ij} = \frac{1}{N} \sum_{n=1}^{N} x_{e_n} (w^E_n)^T\]

<p>This is then incorporated as the edge features into the attention score between nodes via a bias-like term. 
After incorporating the edge and spatial encodings, the value of \(A_{ij}\) is now:</p>

\[A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\phi(v_i,v_j)} + c_{ij}\]

<p>This ensures that edge features directly contribute to the attention score between any two nodes, allowing for a more nuanced and comprehensive utilization of edge information. 
The impact is significant, and it greatly improves the performance, as proven empirically in the Experiments section.</p>

<hr />

<h3 id="vnode">VNode</h3>

<p>The [VNode] (or a Virtual Node) is arguably one of the most important contributions from the work. 
It is an artificial node that is connected to <b>all</b> other nodes. 
The authors cite this paper<d-cite key="gilmer2017neuralmessagepassingquantum"></d-cite> as an empirical motivation, but a better intuition behind the concept is as a generalization of the [CLS] token widely used in NLP and Vision. 
This has an important implication on \(b\) and \(\phi\), because the [VNode] is connected to every node,</p>

\[\phi([VNode], v) = 1, \forall v \in G\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0"> <!--Dummy divs to take up space, need to do this because height, width tags don't work with the given image class-->
    </div>
    <div class="col-sm-6 mt-3 mt-md-0"> <!-- Note  this is a trick to make the image small keep it center but also not too small (using -6)-->
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/vnode3-480.webp 480w,/assets/img/2024-06-30-graphormer/vnode3-800.webp 800w,/assets/img/2024-06-30-graphormer/vnode3-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/vnode3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<p>However, since this is not a <b>physical connection</b>, \(b_{\phi([VNode], v)}\) is set to be a <b>distinct</b> learnable vector (for all \(v\)) to provide the model with this important geometric information.</p>

<p>[CLS] (introduced in <d-cite key="bert">BERT</d-cite>) tokens are often employed as “summary” tokens for text and provide a global context to the model.</p>
<details><summary>More about [CLS] tokens</summary>
<p>In implementation, NLP models have a distinct learnable embedding vector (along with other token embeddings in the mebedding matrix) and append this to the start of every training example, and the final layer representation of this token is used for the task (e.g. sentiment analysis, harmful-ness prediction, etc.).
With enough task-specific (downstream) data the [CLS] token can learn to extract task-relevant information from the data, without having to train the model again!</p>
</details>
<p>With graphs and text being different modalities, the [VNode] also helps in <b>relaying</b> global information to distant or non-connected clusters in a graph. 
This is significantly important to the model’s expressivity, as this information might otherwise never propagate. In fact, the [VNode] becomes a learnable and dataset-specific READOUT function.</p>

<p>This can be implemented as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Initialize the VNode
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">v_node</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="p">...</span>
    <span class="c1"># During forward pass (suppose VNode is the first node)
</span>    <span class="p">...</span>
    <span class="n">headed_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v_node</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">headed_emb</span>
        <span class="c1">#(n_graph, n_heads, n_nodes + 1, n_nodes + 1)
</span>    <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">graph_attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">headed_emb</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>The design choice of one [VNode] per head enables each head to encode different global information.</p>

<hr />

<h2 id="theoretical-aspects-of-expressivity">Theoretical aspects of expressivity</h2>

<p>These are the three main facts from the paper,</p>

<ol>
  <li>With appropriate weights and \(\phi\), GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GraphSAGE<d-cite key="hamilton2018inductiverepresentationlearninglarge"></d-cite>, and GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite> are all <b>special cases</b> of a Graphormer.</li>
  <li>Graphormer is better than architectures that are limited by the 1-WL test. (so <b>all</b> traditional GNNs!)</li>
  <li>With appropriate weights, <b>every node</b> representation in the output can be MEAN-READOUT.</li>
</ol>

<p>The <a href="#spatial-encoding">spatial encoding</a> provides the model with important geometric information. 
Observe that with an appropriate \(b_{\phi(v_i, v_j)}\), the model can <b>find (learn)</b> neighbours for any \(v_i\) and thus easily implement <b>mean-statistics (GCN!)</b>. 
By knowing the degree (some form of <a href="#centrality-encoding">centrality encoding</a>), mean-statistics can be transformed into sum-statistics; it (indirectly) follows that various statistics can be learned by different heads, which leads to varied representations and allows GraphSAGE, GIN or GCN to be modeled as a Graphormer.
We also provide explicit mathematical equations on how the above claims can be realized, feel free to skip them.</p>
<details><summary>Proof(s) for Fact 1</summary>
<p>For each type of aggregation, we provide simple function and weight definitions that achieve it,</p>
<ul>
  <li><b>Mean Aggregate</b> :
    <ul>
      <li>Set $ b_{\phi(v_i, v_j)} = 0 $ when $\phi(v_i, v_j) = 1$ and $-\infty$ otherwise,</li>
      <li>Set $ W_Q = 0, W_K = 0$ and let $ W_V = I$ (Identity matrix), using these,</li>
      <li>
\[h^{(l)}_{v_i} = \sum_{v_j \in N(v_i)} softmax(A_{ij}) * (W_v * h^{(l-1)}_{v_j}) \implies h^{(l)}_{v_i} = \frac{1}{|N(v_i)|}*\sum_{v_j \in N(v_i)} h^{(l-1)}_{v_j}\]
      </li>
    </ul>
  </li>
  <li><b>Sum Aggregate</b> :
    <ul>
      <li>For this, we just need to get the mean aggregate and then multiply by $ |N(v_i)| $,</li>
      <li>Loosely, the degree can be extracted from a <a href="link_to_centrality_eqn">centrality-encoding</a> by an attention head, and then the FFN can multiply this to the learned mean aggregate, the latter part is not so loose, because it is a direct consequence of the universal approximation theorem.</li>
    </ul>
  </li>
  <li><b>Max Aggregate</b> :
    <ul>
      <li>For this one we assume that if we have $t$ dimensions in our hidden state, we <i>also</i> have t heads.</li>
      <li>The proof is such that each Head will extract the maximum from neighbours, clearly, to only keep immediate neighbours around, we can use the same formulation for $b$ and $\phi$ as in the mean aggregate.</li>
      <li>Using $W_K = e_t$ (t-th unit vector), $W_K = e_t$ and $W_Q = 0$ (Identity matrix), we can get a pretty good approximation to the max aggregate. To get the full deal however, we need a <i>hard-max</i> instead of the <i>soft-max</i> being used; to accomplish this we finally consider the bias in the query layer (i.e., something like <code class="language-plaintext highlighter-rouge">nn.Linear(in_dim, out_dim, use_bias=True)</code>), set it to $T \cdot I$ with a high enough $T$ (temperature), this will make the soft-max behave like a hard-max.</li>
    </ul>
  </li>
</ul>
</details>

<p>Fact 2 follows from Fact 1, with GIN being the most powerful traditional GNN, which can theoretically identify all graphs distinguishable by the 1-WL test, as it is now a special case of Graphormer. 
The latter can do the same (&amp; more!).</p>
<details><summary>The WL Test and an example for Fact 2</summary>
<p>First we need to fix some notation for the WL test, briefly, it can be expressed as -</p>

\[c^{(k+1)}(v) = HASH(c^{(k)}(v), \{c^{(k)}(u)\}_{u \in N(v)} )\]

<p>where \(c^{(k)}(v)\) is the \(k^{th}\) iteration representation (color for convinience) of node \(v\) and importantly \(HASH\) is an <i>injective</i> hash function. 
Additionally, all nodes with the same color have the same feature vector</p>

<p>Given this, consider the following graphs -</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/2024-06-30-graphormer/wl-test-480.webp 480w,/assets/img/2024-06-30-graphormer/wl-test-800.webp 800w,/assets/img/2024-06-30-graphormer/wl-test-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/2024-06-30-graphormer/wl-test.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<p>The hashing process converges in one iteration itself, now the 1-WL test would count number of colors and that vector would act as the final graph representation, which for both of the graphs will be \([0, 0, 4, 2]\) (i.e., \([count(a), count(b), count(x), count(y)]\)), even though they are different, the 1-WL test fails to distinguish them. 
There are several such cases and so traditional GNNs are fairly limited in their expressivity.</p>

<p>However for the graphormer, Shortest Path Distances (SPD) directly affects attention weights (because the paper uses SPD as \(\phi(v_i, v_j)\)), and if we look at the SPD sets for the two types of nodes (red and blue) in both the graphs, (we have ordered according to the BFS traversal by top left red node, though any ordering would suffice)</p>

<ul>
  <li>Left graph -
    <ul>
      <li>Red nodes - \(\{ 0, 1, 1, 2, 2, 3 \}\)</li>
      <li>Blue nodes - \(\{1, 0, 2, 1, 1, 2\}\)</li>
    </ul>
  </li>
  <li>Right graph -
    <ul>
      <li>Red nodes - \(\{0, 1, 1, 2, 3, 3\}\)</li>
      <li>Blue nodes - \(\{1, 0, 1, 1, 2, 2\}\)</li>
    </ul>
  </li>
</ul>

<p>What is important is not that red and blue nodes have a different SPD set, <u><i>but that these two types of nodes have different SPD sets across the two graphs</i></u>, this signal can help the model distinguish the two graphs and is the reason why Graphormer is better than 1-WL test limited architectures.</p>
</details>

<p>More importantly, Fact 3 implies that Graphormer allows the flow of <i>Global</i> (and Local) information within the network. 
This truly sets the network apart from traditional GNNs, which can only aggregate local information up to a fixed radius (or depth).</p>

<p>Traditional GNNs are <i>designed</i> to prevent this type of flow, as with their architecture, this would lead to over-smoothening. 
However, the clever design around [VNode] prevents this from happening in Graphormer. 
The addition of a supernode along with Attention and the learnable \(b_{\phi(v_i, v_j)}\) facilitate this, the [VNode] can relay global information, and the attention mechanism can selectively choose from there.</p>

<details><summary>Over-smoothening</summary>
<p>Over-smoothening results in traditional GNNs when the neighbourhood considered for feature aggregation is too large. 
If we build a 10 layer deep network for a graph where the maximum distance bwteen any two nodes is 10, then all nodes will agrregate information from all other nodes, and the final representation will be the same for all nodes.
Thus näively adding a [VNode] / Super Node would lead to over-smoothening in traditional GNNs.</p>
</details>

<p>Operations such as MEAN_READOUT involve aggregation over all nodes, making it a global operation.
Given that Fact 3 implies that every node representation can be MEAN-READOUT, this means that the model can learn to selectively propagate global information using the [VNode].</p>
<details><summary>Proof for Fact 3</summary>
<p>Setting $W_Q = W_K = 0$, and the bias terms in both to be $T \cdot 1$ (where T is temperature), as well as, setting $W_V = I$ (Identity matrix), with a large enough $T$ (much larger than the scale of $b_{\phi(v_i, v_j)}$, so that $T^2 1 1^T$ can dominate), we can get MEAN-READOUT on all nodes. Note that while this proof doesn’t require [VNode], it should be noted that, the [Vnode] is very important to establish a <b>balance</b> between this completely global flow and the local flow. As in a normal setting, with the $T$ not being too large, the only way for global information is through the [VNode], as the $b_{\phi(v_i, v_j)}$ would most likely limit information from nodes that are very far.</p>
</details>

<hr />

<h2 id="experiments">Experiments</h2>

<p>The researchers conducted comprehensive experiments to evaluate Graphormer’s performance against state-of-the-art models like GCN<d-cite key="kipf2017semisupervisedclassificationgraphconvolutional"></d-cite>, GIN<d-cite key="xu2019powerfulgraphneuralnetworks"></d-cite>, DeeperGCN<d-cite key="li2020deepergcnneedtraindeeper"></d-cite>, and the Transformer-based GT<d-cite key="dwivedi2021generalizationtransformernetworksgraphs"></d-cite>.</p>

<p>Two variants of Graphormer, <em>Graphormer</em> (L=12, d=768) and a smaller <em>GraphormerSMALL</em> (L=6, d=512), were evaluated on the <a href="https://ogb.stanford.edu/docs/lsc/">OGB-LSC</a> quantum chemistry regression challenge (PCQM4M-LSC), one of the largest graph-level prediction dataset with over 3.8 million graphs.</p>

<p>The results, as shown in Table 1, demonstrate Graphormer’s significant performance improvements over previous top-performing models such as GIN-VN, DeeperGCN-VN, and GT.</p>

<p>Table 1: Results on PCQM4M-LSC</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Train MAE</th>
      <th>Validate MAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GIN-VN</td>
      <td>6.7M</td>
      <td>0.1150</td>
      <td>0.1395</td>
    </tr>
    <tr>
      <td>DeeperGCN-VN</td>
      <td>25.5M</td>
      <td>0.1059</td>
      <td>0.1398</td>
    </tr>
    <tr>
      <td>GT</td>
      <td>0.6M</td>
      <td>0.0944</td>
      <td>0.1400</td>
    </tr>
    <tr>
      <td>GT-Wide</td>
      <td>83.2M</td>
      <td>0.0955</td>
      <td>0.1408</td>
    </tr>
    <tr>
      <td>GraphormerSMALL</td>
      <td>12.5M</td>
      <td>0.0778</td>
      <td>0.1264</td>
    </tr>
    <tr>
      <td>Graphormer</td>
      <td>47.1M</td>
      <td>0.0582</td>
      <td>0.1234</td>
    </tr>
  </tbody>
</table>

<p>Notably, As pointed out in the <a href="#vnode">VNode</a> section, Graphormer does not encounter over-smoothing issues, with both training and validation errors continuing to decrease as model depth and width increased. 
Additionally, Graph Transformer (GT) showed no performance gain despite a significant increase in parameters from GT to GT-Wide, highlighting Graphormer’s scaling capabilities. 
We also observe a strong overfitting (&gt;0.045 difference between train and validate MAE) for Transformer based models. 
This can be attributed to its special structure, as it can extract more information from the training data.</p>

<details><summary>Graph Trasformer (GT)</summary>
<p>The Graph Transformer is an architecture which works on heterogeneous graphs.
It uses a transformer to create several graphs at runtime by combining several meta-paths and the using a traditional GNN to aggregate information.
As a transformer is not involved in the information relay stage, it’s expressivity is mid-way between a traditional GNN and a Graphormer.</p>

</details>

<p>Further experiments for graph-level prediction tasks were performed on datasets from popular leaderboards like <a href="https://ogb.stanford.edu/docs/graphprop/#ogbg-mol">OGBG</a> (MolPCBA, MolHIV) and <a href="https://paperswithcode.com/paper/benchmarking-graph-neural-networks">benchmarking-GNNs</a> (ZINC), which also showed Graphormer consistently outperforming top-performing GNNs.</p>

<p>By using the ensemble with ExpC<d-cite key="yang2020breakingexpressivebottlenecksgraph"></d-cite>, Graphormer was able to reach a 0.1200 MAE and win the graph-level track in the OGB Large-Scale Challenge.</p>

<h3 id="comparison-against-state-of-the-art-molecular-representation-models">Comparison against State-of-the-Art Molecular Representation Models</h3>

<p>Let’s first take a look at GROVER<d-cite key="rong2020selfsupervisedgraphtransformerlargescale"></d-cite>, a transformer-based GNN boasting 100 million parameters and pre-trained on a massive dataset of 10 million unlabeled molecules.</p>

<p>The authors further fine-tune GROVER on MolHIV and MolPCBA to achieve competitive performance along with supplying additional molecular features such as morgan fingerprints and other 2D features. 
Note that the Random Forest model fitted on these features alone outperforms the GNN model, showing the huge boost in performance granted by the same.</p>

<p>Table 2: Comparison between Graphormer and GROVER on MolHIV</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th># param.</th>
      <th>AUC (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Morgan Finger Prints + Random Forest</td>
      <td>230K</td>
      <td>80.60±0.10</td>
    </tr>
    <tr>
      <td>GROVER</td>
      <td>48.8M</td>
      <td>79.33±0.09</td>
    </tr>
    <tr>
      <td>GROVER (LARGE)</td>
      <td>107.7M</td>
      <td>80.32±0.14</td>
    </tr>
    <tr>
      <td>Graphormer-FLAG</td>
      <td>47.0M</td>
      <td>80.51±0.53</td>
    </tr>
  </tbody>
</table>

<p>However, as evident in Table 2, Graphormer manages to offer competitive performance on the benchmarks without even using the additional features (known to boost performance), which showcases it increases the expressiveness of complex information. 
Additionally the gap in performance can be attributed to the MolHIV dataset, which is too small for Graphormer to extract generalizable features.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Graphormer presents a novel way of applying Transformers to graph representation using the three structural encodings. 
While it has demonstrated strong performance across various benchmark datasets, significant progress has been made since the original paper. 
Structure-Aware Transformer <d-cite key="chen2022structureawaretransformergraphrepresentation"></d-cite> improves on the initial Transformer by incorporating structural information by extracting subgraph representations. 
DeepGraph <d-cite key="zhao2023layersbeneficialgraphtransformers"></d-cite> explores the benefits of deeper graph transformers by enhancing global attention with substructure tokens and local attention. 
Despite the success of these architectures, some challenges still remain; for example, the quadratic complexity of the self-attention module limits its use on large graphs. 
Therefore, the future development of efficient sequence-based graph-processing networks and the imposing of such constraints for geometric learning are open research areas.</p>]]></content><author><name>Tejas Agarwal</name></author><category term="graph" /><category term="representation" /><category term="learning" /><summary type="html"><![CDATA[A first-principles blog post to understand the Graphormer.]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="http://localhost:4000/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+02:00</published><updated>2021-05-22T00:00:00+02:00</updated><id>http://localhost:4000/blog/2021/distill</id><content type="html" xml:base="http://localhost:4000/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/distill-template/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span>
<span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">'assets/distill-template/plotly/demo.html'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="images">Images</h2>

<p>This is an example post with image galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/9-480.webp 480w,/assets/img/distill-template/9-800.webp 800w,/assets/img/distill-template/9-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<p>Images can be made zoomable.
Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/8-480.webp 480w,/assets/img/distill-template/8-800.webp 800w,/assets/img/distill-template/8-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/10-480.webp 480w,/assets/img/distill-template/10-800.webp 800w,/assets/img/distill-template/10-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/11-480.webp 480w,/assets/img/distill-template/11-800.webp 800w,/assets/img/distill-template/11-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/12-480.webp 480w,/assets/img/distill-template/12-800.webp 800w,/assets/img/distill-template/12-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/distill-template/7-480.webp 480w,/assets/img/distill-template/7-800.webp 800w,/assets/img/distill-template/7-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/distill-template/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>
    <p>Unordered list can use asterisks</p>
  </li>
  <li>
    <p>Or minuses</p>
  </li>
  <li>
    <p>Or pluses</p>
  </li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nx">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill" /><category term="formatting" /><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>